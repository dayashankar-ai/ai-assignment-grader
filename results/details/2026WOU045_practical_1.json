{
  "totalScore": 0,
  "baseScore": 0,
  "finalScore": 0,
  "gradeLetter": "F",
  "overallFeedback": "Grading result file not found",
  "criteria": [],
  "strengths": [],
  "improvements": [],
  "studentName": "Bablu Singh",
  "rollNumber": "2026WOU045",
  "email": "bablu.singh@woxsen.edu.in",
  "batch": "2026",
  "practicalNumber": "1",
  "timestamp": "2026-01-30T16:56:13.572Z",
  "aiUsagePercent": 0,
  "submissionText": "STUDENT_NAME: Bablu Singh\nSTUDENT_ID: 2026WOU045\nSTUDENT_EMAIL: bablu.singh@woxsen.edu.in\nBATCH: 2026\nPRACTICAL: 1\nSUBMISSION_DATE: 2026-01-30T16:54:47.588Z\n\n================== CODE/DOCUMENTATION ==================\n\n# Practical 1: Basic Prompt Engineering Exploration\r\n\r\n## Introduction\r\nThis practical explores the fundamentals of prompt engineering for Large Language Models. I will demonstrate various prompt techniques learned in lectures 1-3.\r\n\r\n## Part 1: Understanding Basic Prompt Structure\r\n\r\n### Simple Prompt Analysis\r\nA basic prompt is a straightforward instruction:\r\n```python\r\nbasic_prompt = \"Explain photosynthesis\"\r\n```\r\n\r\n**Issues with this prompt:**\r\n- No specified audience\r\n- No length requirement\r\n- No format specified\r\n\r\n### Improved Prompt with Context\r\n```python\r\nimproved_prompt = \"\"\"\r\nRole: You are a biology teacher.\r\nAudience: 5th grade students\r\nTask: Explain photosynthesis\r\nRequirements:\r\n- Use simple language\r\n- Include an analogy\r\n- Keep it under 100 words\r\n\"\"\"\r\n```\r\n\r\n**Why this is better:**\r\n- Clear role assignment\r\n- Defined audience helps calibrate complexity\r\n- Specific requirements guide output\r\n\r\n## Part 2: Prompt Techniques Demonstrated\r\n\r\n### Technique 1: Role-based Prompting\r\n```python\r\ndef create_role_prompt(role, task):\r\n    return f\"\"\"\r\n    You are a {role}.\r\n    Your task is to {task}.\r\n    Respond in a professional manner appropriate to your role.\r\n    \"\"\"\r\n\r\n# Example usage\r\nprompt1 = create_role_prompt(\"senior Python developer\", \r\n    \"review this code for security vulnerabilities\")\r\n```\r\n\r\n### Technique 2: Few-shot Learning\r\n```python\r\nfew_shot_prompt = \"\"\"\r\nI want you to classify customer feedback as positive, negative, or neutral.\r\n\r\nExamples:\r\n1. \"Great product, love it!\" -> positive\r\n2. \"Terrible experience, never again\" -> negative  \r\n3. \"The product is okay\" -> neutral\r\n\r\nNow classify:\r\n\"This exceeded my expectations!\"\r\n\"\"\"\r\n```\r\n\r\n### Technique 3: Chain of Thought Prompting\r\n```python\r\ncot_prompt = \"\"\"\r\nSolve this step by step:\r\nQuestion: If a train travels 120 km in 2 hours, what is its average speed?\r\n\r\nThink through:\r\n1. First, identify what we know\r\n2. Then, apply the relevant formula\r\n3. Finally, calculate the answer\r\n\r\nShow your reasoning at each step.\r\n\"\"\"\r\n```\r\n\r\n## Part 3: Testing Different Prompt Variations\r\n\r\nI tested the same task with different prompt styles:\r\n\r\n| Prompt Style | Response Quality | Specificity |\r\n|-------------|------------------|-------------|\r\n| Basic | Generic | Low |\r\n| With Context | Better | Medium |\r\n| With Examples | Best | High |\r\n\r\n### Testing Code\r\n```python\r\nimport openai\r\n\r\ndef test_prompt(prompt):\r\n    response = openai.ChatCompletion.create(\r\n        model=\"gpt-3.5-turbo\",\r\n        messages=[{\"role\": \"user\", \"content\": prompt}],\r\n        temperature=0.7\r\n    )\r\n    return response.choices[0].message.content\r\n\r\n# Test different prompts\r\nresults = {}\r\nfor prompt_type, prompt in prompts.items():\r\n    results[prompt_type] = test_prompt(prompt)\r\n```\r\n\r\n## Part 4: Error Handling\r\n```python\r\ndef safe_prompt_request(prompt, max_retries=3):\r\n    for attempt in range(max_retries):\r\n        try:\r\n            return test_prompt(prompt)\r\n        except Exception as e:\r\n            if attempt == max_retries - 1:\r\n                raise\r\n            time.sleep(2 ** attempt)  # Exponential backoff\r\n```\r\n\r\n## Key Learnings\r\n\r\n1. **Specificity matters**: More specific prompts yield more useful responses\r\n2. **Context is crucial**: Providing role and audience improves relevance\r\n3. **Examples help**: Few-shot learning guides the model effectively\r\n4. **Iteration is key**: Refining prompts through testing improves results\r\n\r\n## Challenges Faced\r\n- Initially prompts were too vague\r\n- Had to learn to balance specificity with flexibility\r\n- Understanding token limits and their impact\r\n\r\n## Conclusion\r\nThis practical taught me the fundamentals of effective prompt engineering. I learned that well-structured prompts with clear context and examples produce significantly better results than vague instructions.\r\n\r\n## References\r\n- Course Lectures 1-3\r\n- OpenAI API Documentation\r\n- \"Prompt Engineering Guide\" - Online Resource\n\n================== END OF SUBMISSION ==================\n",
  "instructorComments": ""
}